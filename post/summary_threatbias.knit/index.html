<!DOCTYPE html>
<html lang="en-us">
<head>
	<meta charset="utf-8">
	<meta name="viewport"    content="width=device-width, initial-scale=1.0">
	<meta name="description" content="">
	<meta name="author"      content="Sergey Pozhilov (GetTemplate.com)">

	<title>Clustering Analysis of Dot Probe Data</title>

	<link rel="shortcut icon" href="../../images/gt_favicon.png">

	
	<link href="https://netdna.bootstrapcdn.com/bootstrap/3.0.0/css/bootstrap.no-icons.min.css" rel="stylesheet">
	
	<link href="https://netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">
	
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Alice|Open+Sans:400,300,700">
	
	<link rel="stylesheet" href="../../css/styles.css">

	

    

</head>
<body class="home">

<header id="header">
	<div id="head" >
		<h1 id="logo" class="text-center">
			<img class="img-circle" src="../../images/krista.jpg" alt="">
			<span class="title">Krista L. DeStasio, M.S.</span>
			<span class="tagline"><a href="https://sanlab.uoregon.edu/"><br>
			<span class="tagline"><a href="https://github.com/kdestasio">Visit my GitHub: kdestasio<br>
				<br><a href="mailto:kdestasio@gmail.com">kdestasio@gmail.com</a>
            </span>
		</h1>
	</div>

    <nav class="navbar navbar-default navbar-sticky">
    <div class="container-fluid">

        <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1" aria-expanded="true">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
        </div>

        <div class="navbar-collapse collapse" id="bs-example-navbar-collapse-1">

             <ul class="nav navbar-nav">
                
                <li>
                    <a href="../../">home</a>
                </li>
                
                <li>
                    <a href="../../post/">blog</a>
                </li>
                
                <li>
                    <a href="../../about/">About</a>
                </li>
                
                <li>
                    <a href="../../cv/">CV</a>
                </li>
                
            

        </div>
    </div>
</nav>
<nav class="navbar navbar-fake"></nav>


</header>


<main id="main">

	<div class="container">

		<div class="row topspace">
			<div class="col-sm-8 col-sm-offset-2">

 				<article class="post">
					<header class="entry-header">
 						<div class="entry-meta">
 							<span class="posted-on"><time class="entry-date published" date="2019-09-19 00:00:00 &#43;0000 UTC">September 19, 2019</time></span>
 						</div>
 						<h1 class="entry-title"><a href="../../post/summary_threatbias.knit/" rel="bookmark">Clustering Analysis of Dot Probe Data</a></h1>
					</header>
					<div class="entry-content">
						

<p>This post walks through an exploratory k-means clustering analysis as applied to numerical data from a dot probe task. Steps to preprocess and analyze the data are discussed and the results are presented through visualizations, as well as a discussion of the results.</p>

<p><font size="4"><strong>_All code used in the analysis is available at <a href="https://github.com/kdestasio/hc_analysis">https://github.com/kdestasio/hc_analysis</a>._</strong></font></p>

<h1 id="background-information">Background information</h1>

<p>Analyses were conducted using R (R Core Team, 2018).</p>

<pre><code>platform       x86_64-apple-darwin15.6.0   
arch           x86_64                      
os             darwin15.6.0                
system         x86_64, darwin15.6.0        
status                                     
major          3                           
minor          5.1                         
year           2018                        
month          07                          
day            02                          
svn rev        74947                       
language       R                           
version.string R version 3.5.1 (2018-07-02)
nickname       Feather Spray        
</code></pre>

<h2 id="purpose">Purpose</h2>

<p>The purpose of this analysis is to explore whether meaningful subgroups emerge from performance on an anxiety dot probe (Bar-Haim, Lamy, Pergamin, Bakermans-Kranenburg, &amp; Van Ijzendoorn, 2007; Mathews &amp; Mackintosh, 1998). To conduct this exploratory analysis, we use several clustering algorithms (see Jain, 2010 for an overview of clustering). Clustering algorithms are designed to group data based on their similarity or dissimilarity (e.g. distance in euclidian space). Prior to clustering, dimension reduction is conducted via principal components analysis to decresase overfitting and to make the models more inerpretable (Zha, et al., 2002; Ding &amp; He, 2004). The final solution is selected based on visual inspection of the cluster outcomes, and based on the sum of squared errors for each solution. Outcomes of a spectral clustering analysis (connectivity), a k-means cluster analysis (compactness), and a hierarchical clustering analysis are compared to identify subgroups within the Anxiety Dot Probe measures. These approaches are well suited to numeric data. K-means is a partition based clustering algorithm that groups data points by their euclidian distance from the cluster’s centroid, the mean of that cluster’s objects (Nerurkar et al., 2018). Spectral clustering also uses euclidian distance to group data points, but by representing the data as a similarity graph in which data poionts are nodes and the distances between them are weighted edges (Von Luxburg, 2007). Both k-means and spectral clustering require a priori specification of the number of clusters. Hierarchical clustering, in contrast, creates multiple levels of clusters and subclusters (Cichosz, 2015). As such, hierarchical clustering does not require a priori specification of number of clusters.</p>

<p>This analysis is done with the eventual goal of predicting treatment trajectories in an unrelated sample of Attention Bias Modification Training (ABMT) recipients. In that analysis, we will test whether cluster membership can predict the efficacy of ABMT (toward or away from threat) as measured by a post-ABMT re-application of the anxiety dot-probe.</p>

<h2 id="the-data">The data</h2>

<p>The data are from the first dot probe in a larger, multiwave study by Dr. Tracy Dennis-Tiwary and colleagues (see Figure 1 for a visual of the entire study design). Several different performance measures are calculated for the dot probe to measure bias toward and away from threatening stimuli. The variables used in the clustering analysis are listed below.</p>

<p><img src="images/study_design.png" alt="Figure 1. Study Design" /></p>

<h3 id="dot-probe-variables">Dot probe variables</h3>

<p><strong>Variables included in the analysis</strong></p>

<ul>
<li><code>threatbias_erlab</code>: The average response times (RTs) for neutral probes minus RTs for angry probes on trials with both angry and neutral faces.<br /></li>
<li><code>vigilance_erlab</code>: The average RTs for neutral probes in trials with only neutral images minus RTs for angry probes in trials with both neutral and angry faces.<br /></li>
<li><code>disengage_erlab</code> (removed prior to clustering): The average RTs for neutral probes on trials with both neutral and angry faces minus RTs for neutral probes on trials with only neutral images.<br /></li>
<li><code>rt_threat_nt</code>: The time between a probe appearing on screen and the subsequent button press by a participant.<br /></li>
<li><code>variability</code>: The absolute value of the distance (number of trials between two trials of the same type, e.g. neutral only images) across all trial-level threat bias scores / number of pairs.<br /></li>
</ul>

<h1 id="pre-processing">Pre-processing</h1>

<h2 id="outliers-and-missingness">Outliers and missingness</h2>

<p>Outliers were already removed from the data by our collaborators based on the percentage of trials that were answered correctly. Participants with an accuracy of .80 or greater are included in the dataset. There are no initial missing data for the dot probe measures as the included metrics can be computed for anyone who completed the task.</p>

<h2 id="zero-and-near-zero-variance">Zero and near zero variance</h2>

<p>All variables were checked for zero or near zero variance and no issues were found.</p>

<h2 id="linear-dependencies">Linear dependencies</h2>

<p><code>disengage_erlab</code> is a linear combination of <code>threatbias_erlab</code> and <code>vigilance_erlab</code>. <code>disengage_erlab</code> was therefore not included in the clustering analysis.</p>

<pre><code> disengage = threat_bias - vigilance
</code></pre>

<h2 id="standardization-scaling">Standardization/Scaling</h2>

<p><strong>Purpose: Allow comparison of data that was collected on different scales.</strong></p>

<p>Clustering is sensitive to differences in the measurement scales of data (Mohamad, Usman, 2013). Since the means and standard deviations do vary across features, we standardized them by mean centering Using the <code>scale()</code> function (see Table 1). Variables are scaled to mean = 0, sd = 1.</p>

<p>Table: Table 1. Comparison of Means for Threat Bias Variables Before and After Scaling</p>

<pre><code>                 threatbias_erlab   vigilance_erlab   rt_threat_nt   variability
</code></pre>

<hr />

<p>raw_means_bias                   2.61              1.95         519.65          1.95
raw_sds_bias                    24.75             24.05          81.39          0.94
scaled_means_bias                0.00              0.00           0.00          0.00
scaled_sds_bias                  1.00              1.00           1.00          1.00</p>

<h2 id="dimensionality-reduction-principal-components-analysis">Dimensionality Reduction, Principal Components Analysis</h2>

<p><strong>Purpose: prevent overfitting of the data and improve interpretability</strong></p>

<p>Principal Components Analysis (PCA) is a dimensionality reduction technique that can be used as a precursor to k-means clustering (Zha, et al., 2002; Ding &amp; He, 2004). PCA projects high dimensional data onto a lower dimensional sub-space and rotates data to mazimize variance in the new axes. The components are then listed in order of  decreasing variance (e.g. component 1 has the most variation in the data, component 2 is orthogonal to component 1 and has the most remaining variation, and so on). PCA is an unsuprevised technique that can be used when we do not have hypotheses about the distribution of varaince across features.</p>

<p><img src="images/pca_variance_bias.jpg" alt="" /></p>

<p>Table: Table 2. PCA Summary Table, Threat Bias Dataset</p>

<pre><code>                       PC1    PC2    PC3    PC4
</code></pre>

<hr />

<p>Standard deviation        1.32   1.26   0.63   0.53
Proportion of Variance    0.43   0.40   0.10   0.07
Cumulative Proportion     0.43   0.83   0.93   1.00</p>

<p>Based on the cumulative proportion of variance accounted for by each principal component, the 3-component solution is selected for use in the clustering analysis as it is the point at which more than 90% of the variance in the data can be acounted for (see Table 2).</p>

<h1 id="k-means-clustering">K-means clustering</h1>

<p>K-means is a centroid based clustering approach that groups data points around a central point to minimize within group distance and maximize between group distance. To help determine the number of clusters to use, k-means is run with 2 through 4 clusters.</p>

<h2 id="scatterplots-of-clustering-by-variable">Scatterplots of clustering by variable</h2>

<p><img src="summary_threatbias_files/figure-html/unnamed-chunk-3-1.png" width="672" /><img src="summary_threatbias_files/figure-html/unnamed-chunk-3-2.png" width="672" /><img src="summary_threatbias_files/figure-html/unnamed-chunk-3-3.png" width="672" /></p>

<h2 id="boxplots-by-variable">Boxplots by variable</h2>

<p><img src="images/boxplot_bias_2clusters.jpg" alt="" /></p>

<p><img src="images/boxplot_bias_3clusters.jpg" alt="" /></p>

<p><img src="images/boxplot_bias_4clusters.jpg" alt="" /></p>

<h2 id="silhouette-plots">Silhouette plots</h2>

<p><img src="images/silplot_pca3_bias_2clusters.png" alt="" /></p>

<p><img src="images/silplot_pca3_bias_3clusters.png" alt="" /></p>

<p><img src="images/silplot_pca3_bias_4clusters.png" alt="" /></p>

<h1 id="summary">Summary</h1>

<p>The clustering analysis was performed using the threat bias variables <code>threatbias_erlab</code> and <code>vigilance_erlab</code>, as well as the response time variable <code>rt_threat_nt</code> and the variability variable <code>variability</code>. As mentioned in the preprocessing section, <code>disengage_erlab</code> does not provide useful information for development of a clustering solution, nor its interpretation as the variable is a linear combination of <code>threatbias_erlab</code> and <code>vigilance_erlab</code>. Response time is the predominent feature on which data are clustered.</p>

<p>Based on the scatterplots of each variable, the data <strong>do not exhibit any natural clustering tendencies.</strong> However, we discuss differences between the &ldquo;best&rdquo; clustering solutions to illustrate how one might decide between several possible clustering solutions.</p>

<p>The main difference between clusters within solutions appears to relate to response time and threat bias scores, as evidenced by the boxplots and scatterplots (see above plots). As can be seen in the silhouette plots, cluster 1 is the largest in all of the clustering solutions, reflecting generally faster response times. In the 2 cluster solution, cluster 1 members have a generally faster response time and higher threat bias scores. Cluster 2 of the 2 cluster solution contains observations with generally slower response times and lower threat bias scores. Cluster 1 is most people. In the 3 cluster solution, there is also a small 3rd cluster that consists of slower response times and higher threat bias scores. Both the 2 and 3 cluster solutions are equally “good”. The 4 cluster solution  appears to break out the fourth cluster based on long response times. Any of the solutions are viable options, though the 2 and 3 cluster solutions are likely more easily interpretable and the fewer clusters there are the fewer observations fit poorly within them (see the number of negative silhouette observations in table 3). Overall clustering solution fit measures are presented in table 3 and see table 4 for summary measures of fit by cluster within each clustering solution.</p>

<p>Table: Table 3. Clustering Solution Summary Statistics: by Clustering Solition</p>

<p>clustering_solution   total_within_cluster_ss   total_between_cluster_ss   average_silhouette_coefficient   negative_silhouette</p>

<hr />

<pre><code>               2                   2229.00                     910.31                             0.39                    28
               3                   1610.11                    1529.20                             0.36                    43
               4                   1347.36                    1791.96                             0.31                    66
</code></pre>

<p>Table: Table 4. Clustering Solution Summary Statistics: by Cluster, Within Clustering Solition</p>

<p>clustering_solution   cluster   observations_per_cluster   within_cluster_ss   average_silhouette_width</p>

<hr />

<pre><code>               2         1                        651             1384.47                       0.45
               2         2                        193              844.53                       0.18
               3         1                        546              635.01                       0.45
               3         2                        205              454.64                       0.25
               3         3                         93              520.46                       0.13
               4         1                        446              381.52                       0.41
               4         2                         36              221.68                       0.20
               4         3                        192              438.25                       0.21
               4         4                        170              305.90                       0.19
</code></pre>

<h1 id="references">References</h1>

<p>Bar-Haim, Y., Lamy, D., Pergamin, L., Bakermans-Kranenburg, M. J., &amp; Van Ijzendoorn, M. H. (2007). Threat-related attentional bias in anxious and nonanxious individuals: a meta-analytic study. Psychological bulletin, 133(1), 1.</p>

<p>Cichosz, P. (2015). Data mining algorithms: explained using R. John Wiley &amp; Sons Incorporated.</p>

<p>Ding, C., &amp; He, X. (2004, July). K-means clustering via principal component analysis. In Proceedings of the twenty-first international conference on Machine learning (p. 29). ACM.</p>

<p>Jain, A. K. (2010). Data clustering: 50 years beyond K-means. Pattern recognition letters, 31(8), 651-666.</p>

<p>Mathews, A., &amp; Mackintosh, B. (1998). A cognitive model of selective processing in anxiety. Cognitive therapy and research, 22(6), 539-560.</p>

<p>Mohamad, I. B., &amp; Usman, D. (2013). Standardization and its effects on K-means clustering algorithm. Research Journal of Applied Sciences, Engineering and Technology, 6(17), 3299-3303.</p>

<p>Nerurkar, P., Shirke, A., Chandane, M., &amp; Bhirud, S. (2018). Empirical analysis of data clustering algorithms. Procedia Computer Science, 125, 770-779.</p>

<p>Von Luxburg, U. (2007). A tutorial on spectral clustering. Statistics and computing, 17(4), 395-416.</p>

<p>Zha, Hongyuan, Xiaofeng He, Chris Ding, Ming Gu, and Horst D Simon. “Spectral Relaxation for K-Means Clustering.” Advances in Neural Information Processing Systems, 2002, 1057–64.</p>

<h1 id="see-also">See also</h1>

<p>This fun clustering application that will let you play with some pokemon data and kmeans clustering: <a href="https://kdestasio.shinyapps.io/fpr_final_project/">https://kdestasio.shinyapps.io/fpr_final_project/</a></p>

					</div>
				</article>

			</div>
		</div> 

        <div class="row">
			<div class="col-sm-8 col-sm-offset-2">

				<div id="share">
                    
				</div>
			</div>
		</div> 
		<div class="clearfix"></div>

		<div class="row">
			<div class="col-sm-8 col-sm-offset-2">

				<div id="comments">
                    
				</div>
			</div>
		</div> 
		<div class="clearfix"></div>

	</div>	

</main>

<footer id="footer">
	<div class="container">
		<div class="row">
			
			<div class="col-md-3 widget">
				<h3 class="widget-title">Contact</h3>
				<div class="widget-body">
					<p>
					<a href="mailto:kdestasio@gmail.com">kdestasio@gmail.com</a><br>
						<br>
						University of Oregon
					</p>
				</div>
			</div>
			

			
			<div class="col-md-3 widget">
				<h3 class="widget-title">Follow me</h3>
				<div class="widget-body">
					<p class="follow-me-icons">
                        
                            
                        
                            
                                <a href="https://twitter.com/psychneurd" target="_blank"><i class="fa fa-twitter-square fa-2"></i></a>
                            
                        
                            
                        
                            
                        
                            
                                <a href="https://www.linkedin.com/in/krista-destasio" target="_blank"><i class="fa fa-linkedin-square fa-2"></i></a>
                            
                        
                            
                        
                            
                                <a href="https://github.com/kdestasio" target="_blank"><i class="fa fa-github fa-2"></i></a>
                            
                        
                            
                                <a href="mailto:kdestasi@uoregon.edu" target="_blank"><i class="fa fa-envelope fa-2"></i></a>
                            
                        
					</p>
				</div>
			</div>
			

			

			

		</div> 
	</div>
</footer>

<footer id="underfooter">
	<div class="container">
		<div class="row">

			<div class="col-md-6 widget">
				<div class="widget-body">
					<p>University of Oregon</p>
				</div>
			</div>

			<div class="col-md-6 widget">
				<div class="widget-body">
					<p class="text-right">
						Copyright &copy; 2019, Krista L. DeStasio, M.S.<br>
						Design: <a href="http://www.gettemplate.com" rel="designer">Initio by GetTemplate</a> - 
                        Powered by: <a href="https://gohugo.io/" rel="poweredby">Hugo</a>
                    </p>
				</div>
			</div>

		</div> 
	</div>
</footer>




<script src="https://code.jquery.com/jquery-1.12.4.min.js"></script>
<script src="https://netdna.bootstrapcdn.com/bootstrap/3.0.0/js/bootstrap.min.js"></script>
<script src="../../js/template.js"></script>
<script id="dsq-count-scr" src="//hugo-initio-site.disqus.com/count.js" async></script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', '', 'auto');
  ga('send', 'pageview');
</script>

</body>
</html>

